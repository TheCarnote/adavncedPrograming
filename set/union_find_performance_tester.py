import random
import time
import string
import hashlib
import json
from collections import defaultdict
from typing import Iterable, Hashable, Tuple, List, Dict, Set, Any
import collections


# --- Fonctions √† tester (TODO: Impl√©mentez vos solutions ici) ---

def partition_par_unions_naive(
        elements: Iterable[Hashable],
        relations: Iterable[Tuple[Hashable, Hashable]]
) -> List[List[Hashable]]:
    """
    Impl√©mentation vraiment na√Øve (brute force) n'utilisant que des listes.
    La recherche d'un √©l√©ment se fait par balayage de la liste de groupes.
    """
    # √âtape 1 : Initialisation
    partition = [[elem] for elem in elements]

    # Cr√©er une liste pour v√©rifier rapidement si un √©l√©ment existe
    elements_set = set(elements)

    # √âtape 2 : Parcourir les relations et effectuer les fusions
    for x, y in relations:
        # V√©rifier l'existence des √©l√©ments
        if x not in elements_set or y not in elements_set:
            continue

        group_x = None
        group_y = None

        # Trouver les groupes de x et y par balayage (recherche lente)
        for group in partition:
            if x in group:
                group_x = group
            if y in group:
                group_y = group
            # On peut optimiser la boucle en sortant plus t√¥t si les deux sont trouv√©s
            if group_x and group_y:
                break

        # S'assurer que les deux √©l√©ments sont trouv√©s (devrait toujours l'√™tre)
        if group_x is None or group_y is None:
            continue

        # √âtape 3 : Fusionner si les groupes sont diff√©rents
        if group_x is not group_y:
            group_x.extend(group_y)
            # Retirer le groupe fusionn√© de la partition
            partition.remove(group_y)

    # √âtape 4 : Retourner la partition finale
    return partition


# Les 5 fonctions "placeholders" demand√©es
def partition_par_unions_1(
        elements: Iterable[Hashable],
        relations: Iterable[Tuple[Hashable, Hashable]]
) -> List[List[Hashable]]:
    """TODO: impl√©mentez votre premi√®re solution ici."""
    """
       Impl√©mentation acad√©mique utilisant des sets.
       """
    """
    Impl√©mentation corrig√©e utilisant des sets avec mapping direct vers les sets.
    """
    elements = list(elements)
    elements_set = set(elements)

    # Chaque √©l√©ment pointe vers son set actuel
    elem_to_group = {}
    groups = []

    # Initialisation: chaque √©l√©ment dans son propre groupe
    for elem in elements:
        group = {elem}
        groups.append(group)
        elem_to_group[elem] = group

    for x, y in relations:
        if x not in elements_set or y not in elements_set:
            continue

        group_x = elem_to_group[x]
        group_y = elem_to_group[y]

        if group_x is not group_y:
            # Fusionner group_y dans group_x (le plus grand)
            if len(group_y) > len(group_x):
                group_x, group_y = group_y, group_x

            group_x.update(group_y)

            # Mettre √† jour toutes les r√©f√©rences vers group_y
            for elem in group_y:
                elem_to_group[elem] = group_x

            # Retirer group_y de la liste des groupes
            groups.remove(group_y)

    return [list(group) for group in groups]

def partition_par_unions_2(
        elements: Iterable[Hashable],
        relations: Iterable[Tuple[Hashable, Hashable]]
) -> List[List[Hashable]]:
    """TODO: impl√©mentez votre deuxi√®me solution ici."""
    """
    Impl√©mentation corrig√©e utilisant un dictionnaire pour mapper vers les repr√©sentants.
    """
    elements = list(elements)
    elements_set = set(elements)

    # Chaque √©l√©ment est son propre repr√©sentant au d√©but
    representative = {elem: elem for elem in elements}
    groups = {elem: [elem] for elem in elements}

    def find_representative(x):
        """Trouve le repr√©sentant du groupe contenant x."""
        return representative[x]

    def union(x, y):
        """Fusionne les groupes contenant x et y."""
        rep_x = find_representative(x)
        rep_y = find_representative(y)

        if rep_x == rep_y:
            return

        # Fusionner le plus petit groupe dans le plus grand
        group_x = groups[rep_x]
        group_y = groups[rep_y]

        if len(group_y) > len(group_x):
            rep_x, rep_y = rep_y, rep_x
            group_x, group_y = group_y, group_x

        # Ajouter tous les √©l√©ments de group_y √† group_x
        group_x.extend(group_y)

        # Mettre √† jour le repr√©sentant de tous les √©l√©ments de group_y
        for elem in group_y:
            representative[elem] = rep_x

        # Supprimer l'ancien groupe
        del groups[rep_y]

    for x, y in relations:
        if x in elements_set and y in elements_set:
            union(x, y)

    return list(groups.values())


def partition_par_unions_3(
        elements: Iterable[Hashable],
        relations: Iterable[Tuple[Hashable, Hashable]]
) -> List[List[Hashable]]:
    """
        Union-Find (Weighted Quick-Union) :
        - Union par taille (attache l'arbre le plus petit au plus grand)
        - PAS de compression de chemin (volontairement, pour rester "optimis√© mais pas au max")

        Complexit√© (amortie) : ~ O(m log n)
          - n = nombre d'√©l√©ments
          - m = nombre de relations valides (dont les deux extr√©mit√©s ‚àà elements)

        Politique :
          - Ignore toute relation (x, y) si x ou y n'appartient pas √† `elements`.
          - Retourne une liste de listes (ordre ind√©termin√©, ce qui convient √† compare_results).
        """

    # Mat√©riel de base
    elements = list(elements)
    elements_set = set(elements)

    # Parent et taille des composantes (repr√©sentation par racines)
    parent: Dict[Hashable, Hashable] = {e: e for e in elements}
    size: Dict[Hashable, int] = {e: 1 for e in elements}

    def find(x: Hashable) -> Hashable:
        """Trouve la racine de x (sans compression de chemin)."""
        # x est suppos√© ‚àà elements_set avant l'appel
        while parent[x] != x:
            x = parent[x]
        return x

    def union(x: Hashable, y: Hashable) -> None:
        """Fusionne les composantes contenant x et y en utilisant l'union par taille."""
        rx, ry = find(x), find(y)
        if rx == ry:
            return
        # Attache le plus petit au plus grand
        if size[rx] < size[ry]:
            parent[rx] = ry
            size[ry] += size[rx]
        else:
            parent[ry] = rx
            size[rx] += size[ry]

    # Appliquer les relations (en ignorant celles hors domaine)
    for (x, y) in relations:
        if (x in elements_set) and (y in elements_set):
            union(x, y)

    # Rassembler les groupes par racine
    groups: Dict[Hashable, List[Hashable]] = {}
    for e in elements:
        r = find(e)  # sans compression pour rester coh√©rent avec le choix de l'algo
        if r not in groups:
            groups[r] = []
        groups[r].append(e)

    return list(groups.values())


def partition_par_unions_4(
        elements: Iterable[Hashable],
        relations: Iterable[Tuple[Hashable, Hashable]]
) -> List[List[Hashable]]:
    """
        Union-Find avec compression de chemin ET union par rang.
        Complexit√© amortie: O(Œ±(n)) par op√©ration, o√π Œ± est l'inverse d'Ackermann.
        """
    elements = list(elements)
    elements_set = set(elements)

    parent = {elem: elem for elem in elements}
    rank = {elem: 0 for elem in elements}

    def find(x):
        """Trouve la racine avec compression de chemin."""
        if parent[x] != x:
            parent[x] = find(parent[x])  # Compression de chemin
        return parent[x]

    def union(x, y):
        """Union par rang."""
        root_x = find(x)
        root_y = find(y)

        if root_x == root_y:
            return

        # Union par rang: attacher le plus petit au plus grand
        if rank[root_x] < rank[root_y]:
            parent[root_x] = root_y
        elif rank[root_x] > rank[root_y]:
            parent[root_y] = root_x
        else:
            parent[root_y] = root_x
            rank[root_x] += 1

    # Traiter toutes les relations
    for x, y in relations:
        if x in elements_set and y in elements_set:
            union(x, y)

    # Construire les groupes finaux
    groups = {}
    for elem in elements:
        root = find(elem)
        if root not in groups:
            groups[root] = []
        groups[root].append(elem)

    return list(groups.values())


def partition_par_unions_5(
        elements: Iterable[Hashable],
        relations: Iterable[Tuple[Hashable, Hashable]]
) -> List[List[Hashable]]:
    """
        Approche bas√©e sur les graphes avec DFS pour trouver les composantes connexes.
        """
    elements = list(elements)
    elements_set = set(elements)

    # Construire le graphe d'adjacence
    graph = {elem: set() for elem in elements}

    for x, y in relations:
        if x in elements_set and y in elements_set:
            graph[x].add(y)
            graph[y].add(x)

    visited = set()
    groups = []

    def dfs(node, current_group):
        """DFS pour explorer une composante connexe."""
        if node in visited:
            return
        visited.add(node)
        current_group.append(node)

        for neighbor in graph[node]:
            if neighbor not in visited:
                dfs(neighbor, current_group)

    # Trouver toutes les composantes connexes
    for elem in elements:
        if elem not in visited:
            group = []
            dfs(elem, group)
            groups.append(group)

    return groups


# --- Fonctions utilitaires pour la g√©n√©ration de donn√©es et la comparaison ---

def generate_random_chars(length: int = 4) -> str:
    """G√©n√®re une cha√Æne de caract√®res al√©atoires lisibles."""
    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))


def generate_test_data(n_elements: int, n_relations: int) -> Tuple[List[str], List[Tuple[str, str]]]:
    """
    G√©n√®re des √©l√©ments et des relations de test selon les sp√©cifications.
    Les √©l√©ments g√©n√©r√©s sont garantis d'√™tre uniques.
    """
    print(f"G√©n√©ration des donn√©es : {n_elements} √©l√©ments, {n_relations} relations.")

    # Utiliser un set pour garantir l'unicit√© des √©l√©ments
    unique_elements = set()
    while len(unique_elements) < n_elements:
        unique_elements.add(generate_random_chars())
    elements = list(unique_elements)

    relations = []

    n_valid_relations = n_relations // 2
    if n_elements < 2:
        n_valid_relations = 0

    # Cr√©er les relations valides √† partir des √©l√©ments uniques
    for _ in range(n_valid_relations):
        x, y = random.sample(elements, 2)
        relations.append((x, y))

    # Cr√©er les relations al√©atoires
    n_random_relations = n_relations - n_valid_relations
    for _ in range(n_random_relations):
        x = generate_random_chars()
        y = generate_random_chars()
        relations.append((x, y))

    random.shuffle(relations)
    return elements, relations


def compare_results(result1: List[List[Hashable]], result2: List[List[Hashable]]) -> bool:
    """Compare deux partitions de groupes, ind√©pendamment de l'ordre."""

    def normalize_partition(partition):
        return sorted([tuple(sorted(group)) for group in partition])

    try:
        return normalize_partition(result1) == normalize_partition(result2)
    except Exception as e:
        print(f"Erreur lors de la normalisation des r√©sultats : {e}")
        return False


def run_tests(
        implementations: Dict[str, callable],
        n_elements_list: List[int],
        n_relations_factors: List[float]
) -> None:
    """
    Ex√©cute les tests de performance et de validit√©.
    """
    print("üöÄ D√©but des tests de performance et de validit√©.")
    print("-" * 50)

    for i, n_elements in enumerate(n_elements_list):
        for j, factor in enumerate(n_relations_factors):
            # Correction ici pour le calcul du nombre de relations
            if factor == 1000:
                n_relations = 1000
            else:
                n_relations = max(1000, int(n_elements * factor)) if n_elements > 0 else 1000

            elements, relations = generate_test_data(n_elements, n_relations)

            print(f"\nüß™ Test {i + 1}.{j + 1} : {n_elements} √©l√©ments, {n_relations} relations")

            print("  -> Ex√©cution de la r√©f√©rence (na√Øve)... ", end="")
            start_time = time.time()
            ref_result = partition_par_unions_naive(elements, relations)
            end_time = time.time()
            print(f"Termin√© en {end_time - start_time:.4f}s")

            for name, func in implementations.items():
                print(f"  -> Ex√©cution de '{name}'... ", end="")
                try:
                    start_time = time.time()
                    current_result = func(elements, relations)
                    end_time = time.time()
                    duration = end_time - start_time

                    is_correct = compare_results(ref_result, current_result)
                    status = "‚úÖ Correct" if is_correct else "‚ùå Erreur"
                    print(f"Termin√© en {duration:.4f}s ({status})")

                    if not is_correct:
                        print(f"     Les r√©sultats de '{name}' sont incorrects pour ce test.")
                except Exception as e:
                    print(f"Erreur lors de l'ex√©cution de '{name}': {e}")

            print("-" * 50)

    print("\n‚úÖ Tous les tests sont termin√©s.")

#--------

def hash_partition(partition: List[List[Any]]) -> str:
    """Cr√©e un hash unique pour une partition, ind√©pendamment de l'ordre."""
    try:
        # Normaliser: trier chaque groupe et trier les groupes
        normalized = sorted([tuple(sorted(group)) for group in partition])
        # Cr√©er un hash reproductible
        return hashlib.md5(str(normalized).encode()).hexdigest()[:8]
    except Exception as e:
        return f"ERROR_{str(e)[:8]}"


def analyze_partition(partition: List[List[Any]], name: str) -> Dict[str, Any]:
    """Analyse d√©taill√©e d'une partition."""
    try:
        # Statistiques de base
        num_groups = len(partition)
        total_elements = sum(len(group) for group in partition)
        group_sizes = [len(group) for group in partition]

        # V√©rifier la validit√©
        all_elements = set()
        has_duplicates = False
        empty_groups = 0

        for group in partition:
            if len(group) == 0:
                empty_groups += 1
            for elem in group:
                if elem in all_elements:
                    has_duplicates = True
                all_elements.add(elem)

        return {
            'name': name,
            'hash': hash_partition(partition),
            'num_groups': num_groups,
            'total_elements': total_elements,
            'group_sizes': sorted(group_sizes, reverse=True),
            'max_group_size': max(group_sizes) if group_sizes else 0,
            'min_group_size': min(group_sizes) if group_sizes else 0,
            'empty_groups': empty_groups,
            'has_duplicates': has_duplicates,
            'is_valid': not has_duplicates and empty_groups == 0,
            'partition': partition  # Pour debug si n√©cessaire
        }
    except Exception as e:
        return {
            'name': name,
            'error': str(e),
            'is_valid': False
        }


def detailed_comparison_analysis(
        elements: List[Any],
        relations: List[Tuple[Any, Any]],
        implementations: Dict[str, callable]
) -> Dict[str, Any]:
    """Analyse comparative d√©taill√©e de toutes les impl√©mentations."""

    print(f"\nüîç ANALYSE D√âTAILL√âE")
    print(f"üìä Donn√©es: {len(elements)} √©l√©ments, {len(relations)} relations")
    print(f"üîó Relations valides: {sum(1 for x, y in relations if x in set(elements) and y in set(elements))}")
    print("=" * 60)

    results = {}
    analyses = []

    # Ex√©cuter toutes les impl√©mentations
    for name, func in implementations.items():
        try:
            start_time = time.time()
            result = func(elements, relations)
            duration = time.time() - start_time

            analysis = analyze_partition(result, name)
            analysis['duration'] = duration
            analyses.append(analysis)
            results[name] = result

        except Exception as e:
            analyses.append({
                'name': name,
                'error': str(e),
                'is_valid': False,
                'duration': 0
            })

    # Grouper par hash (r√©sultats identiques)
    hash_groups = defaultdict(list)
    for analysis in analyses:
        if 'hash' in analysis:
            hash_groups[analysis['hash']].append(analysis)

    # Afficher l'analyse
    print("\nüìã R√âSULTATS PAR IMPL√âMENTATION:")
    print("-" * 60)
    for analysis in sorted(analyses, key=lambda x: x.get('duration', 0)):
        name = analysis['name']
        if 'error' in analysis:
            print(f"‚ùå {name:15s} | ERREUR: {analysis['error']}")
        else:
            valid_icon = "‚úÖ" if analysis['is_valid'] else "‚ùå"
            print(f"{valid_icon} {name:15s} | {analysis['duration']:.4f}s | "
                  f"Hash: {analysis['hash']} | "
                  f"Groupes: {analysis['num_groups']:4d} | "
                  f"√âl√©ments: {analysis['total_elements']:5d}")

            if not analysis['is_valid']:
                issues = []
                if analysis['has_duplicates']:
                    issues.append("doublons")
                if analysis['empty_groups'] > 0:
                    issues.append(f"{analysis['empty_groups']} groupes vides")
                print(f"   ‚îî‚îÄ PROBL√àMES: {', '.join(issues)}")

    # Analyser les groupes de r√©sultats identiques
    print(f"\nüéØ GROUPES DE R√âSULTATS IDENTIQUES:")
    print("-" * 60)

    consensus_found = False
    largest_group_size = 0
    largest_group_hash = None

    for hash_val, group in hash_groups.items():
        if len(group) > 1:  # Plus d'une impl√©mentation avec le m√™me r√©sultat
            print(f"üì¶ Hash {hash_val} ({len(group)} impl√©mentations):")
            for analysis in group:
                valid_icon = "‚úÖ" if analysis.get('is_valid', False) else "‚ùå"
                print(f"   {valid_icon} {analysis['name']}")

            if len(group) > largest_group_size:
                largest_group_size = len(group)
                largest_group_hash = hash_val

            # Montrer les d√©tails du premier r√©sultat du groupe
            first = group[0]
            if 'group_sizes' in first:
                print(f"   ‚îî‚îÄ D√©tails: {first['num_groups']} groupes, "
                      f"tailles: {first['group_sizes'][:5]}{'...' if len(first['group_sizes']) > 5 else ''}")

    # R√©sultats uniques
    unique_results = [group[0] for group in hash_groups.values() if len(group) == 1]
    if unique_results:
        print(f"\nüî∏ R√âSULTATS UNIQUES ({len(unique_results)}):")
        for analysis in unique_results:
            valid_icon = "‚úÖ" if analysis.get('is_valid', False) else "‚ùå"
            print(f"   {valid_icon} {analysis['name']} (Hash: {analysis.get('hash', 'N/A')})")

    # Recommandations
    print(f"\nüí° ANALYSE ET RECOMMANDATIONS:")
    print("-" * 60)

    if largest_group_size > 1:
        print(
            f"‚ú® CONSENSUS D√âTECT√â: {largest_group_size} impl√©mentations donnent le m√™me r√©sultat (Hash: {largest_group_hash})")
        consensus_group = [a for a in analyses if a.get('hash') == largest_group_hash]
        if all(a.get('is_valid', False) for a in consensus_group):
            print("   ‚îî‚îÄ Ce r√©sultat semble VALIDE (pas de doublons, pas de groupes vides)")
        else:
            print("   ‚îî‚îÄ ATTENTION: Ce r√©sultat a des probl√®mes de validit√©")
    else:
        print("‚ö†Ô∏è  AUCUN CONSENSUS: Toutes les impl√©mentations donnent des r√©sultats diff√©rents")

    # Identifier les impl√©mentations suspectes
    invalid_impls = [a for a in analyses if not a.get('is_valid', True)]
    if invalid_impls:
        print(f"\n‚ùå IMPL√âMENTATIONS AVEC ERREURS ({len(invalid_impls)}):")
        for analysis in invalid_impls:
            print(f"   - {analysis['name']}")

    return {
        'analyses': analyses,
        'hash_groups': dict(hash_groups),
        'consensus_hash': largest_group_hash,
        'consensus_size': largest_group_size,
        'total_implementations': len(analyses),
        'valid_implementations': sum(1 for a in analyses if a.get('is_valid', True))
    }


def run_single_test_with_analysis(
        elements: List[Any],
        relations: List[Tuple[Any, Any]],
        implementations: Dict[str, callable]
) -> None:
    """Ex√©cute un test unique avec analyse d√©taill√©e."""
    analysis_result = detailed_comparison_analysis(elements, relations, implementations)

    # Suggestions d'actions
    print(f"\nüéØ SUGGESTIONS D'ACTION:")
    print("-" * 60)

    if analysis_result['consensus_size'] >= 3:
        consensus_hash = analysis_result['consensus_hash']
        consensus_impls = [analysis['name'] for analysis in analysis_result['hash_groups'][consensus_hash]]
        print(f"1. Le consensus ({consensus_impls}) est probablement CORRECT")
        print("2. Investiguer les impl√©mentations divergentes")
    elif analysis_result['consensus_size'] == 2:
        print("1. Deux impl√©mentations concordent - v√©rifier manuellement")
        print("2. Les autres impl√©mentations sont probablement incorrectes")
    else:
        print("1. SITUATION CRITIQUE: Aucun consensus d√©tect√©")
        print("2. V√©rifier la logique de TOUTES les impl√©mentations")
        print("3. Faire un test manuel avec un petit jeu de donn√©es")


# Fonction modifi√©e pour les tests
def run_tests_with_detailed_analysis(
        implementations: Dict[str, callable],
        n_elements_list: List[int],
        n_relations_factors: List[float],
        detailed_analysis_frequency: int = 5  # Analyse d√©taill√©e tous les N tests
) -> None:
    """
    Version am√©lior√©e de run_tests avec analyse d√©taill√©e p√©riodique.
    """
    print("üöÄ D√©but des tests de performance et de validit√© AVEC ANALYSE D√âTAILL√âE.")
    print("-" * 70)

    test_counter = 0

    for i, n_elements in enumerate(n_elements_list):
        for j, factor in enumerate(n_relations_factors):
            test_counter += 1

            # Calcul du nombre de relations
            if factor == 1000:
                n_relations = 1000
            else:
                n_relations = max(1000, int(n_elements * factor)) if n_elements > 0 else 1000

            elements, relations = generate_test_data(n_elements, n_relations)

            print(f"\nüß™ Test {i + 1}.{j + 1} : {n_elements} √©l√©ments, {n_relations} relations")

            # Analyse d√©taill√©e p√©riodiquement ou si on d√©tecte des erreurs
            should_analyze = (test_counter % detailed_analysis_frequency == 1)

            if not should_analyze:
                # Test rapide normal
                print("  -> Ex√©cution de la r√©f√©rence (na√Øve)... ", end="")
                start_time = time.time()
                ref_result = partition_par_unions_naive(elements, relations)
                end_time = time.time()
                print(f"Termin√© en {end_time - start_time:.4f}s")

                error_count = 0
                for name, func in implementations.items():
                    if name == 'na√Øve':  # Skip duplicate
                        continue
                    print(f"  -> Ex√©cution de '{name}'... ", end="")
                    try:
                        start_time = time.time()
                        current_result = func(elements, relations)
                        end_time = time.time()
                        duration = end_time - start_time

                        is_correct = compare_results(ref_result, current_result)
                        status = "‚úÖ Correct" if is_correct else "‚ùå Erreur"
                        print(f"Termin√© en {duration:.4f}s ({status})")

                        if not is_correct:
                            error_count += 1
                    except Exception as e:
                        print(f"Erreur lors de l'ex√©cution de '{name}': {e}")
                        error_count += 1

                # Si beaucoup d'erreurs, faire une analyse d√©taill√©e
                if error_count >= len(implementations) // 2:
                    print(f"\n‚ö†Ô∏è  TROP D'ERREURS D√âTECT√âES ({error_count}) - ANALYSE D√âTAILL√âE:")
                    should_analyze = True

            if should_analyze:
                # Analyse d√©taill√©e
                run_single_test_with_analysis(elements, relations, implementations)

            print("-" * 70)

    print("\n‚úÖ Tous les tests sont termin√©s.")

# Fonction modifi√©e pour les tests
def run_tests_with_detailed_analysis(
        implementations: Dict[str, callable],
        n_elements_list: List[int],
        n_relations_factors: List[float],
        detailed_analysis_frequency: int = 5  # Analyse d√©taill√©e tous les N tests
) -> None:
    """
    Version am√©lior√©e de run_tests avec analyse d√©taill√©e p√©riodique.
    """
    print("üöÄ D√©but des tests de performance et de validit√© AVEC ANALYSE D√âTAILL√âE.")
    print("-" * 70)

    test_counter = 0

    for i, n_elements in enumerate(n_elements_list):
        for j, factor in enumerate(n_relations_factors):
            test_counter += 1

            # Calcul du nombre de relations
            if factor == 1000:
                n_relations = 1000
            else:
                n_relations = max(1000, int(n_elements * factor)) if n_elements > 0 else 1000

            elements, relations = generate_test_data(n_elements, n_relations)

            print(f"\nüß™ Test {i + 1}.{j + 1} : {n_elements} √©l√©ments, {n_relations} relations")

            # Analyse d√©taill√©e p√©riodiquement ou si on d√©tecte des erreurs
            should_analyze = (test_counter % detailed_analysis_frequency == 1)

            if not should_analyze:
                # Test rapide normal
                print("  -> Ex√©cution de la r√©f√©rence (na√Øve)... ", end="")
                start_time = time.time()
                ref_result = partition_par_unions_naive(elements, relations)
                end_time = time.time()
                print(f"Termin√© en {end_time - start_time:.4f}s")

                error_count = 0
                for name, func in implementations.items():
                    if name == 'na√Øve':  # Skip duplicate
                        continue
                    print(f"  -> Ex√©cution de '{name}'... ", end="")
                    try:
                        start_time = time.time()
                        current_result = func(elements, relations)
                        end_time = time.time()
                        duration = end_time - start_time

                        is_correct = compare_results(ref_result, current_result)
                        status = "‚úÖ Correct" if is_correct else "‚ùå Erreur"
                        print(f"Termin√© en {duration:.4f}s ({status})")

                        if not is_correct:
                            error_count += 1
                    except Exception as e:
                        print(f"Erreur lors de l'ex√©cution de '{name}': {e}")
                        error_count += 1

                # Si beaucoup d'erreurs, faire une analyse d√©taill√©e
                if error_count >= len(implementations) // 2:
                    print(f"\n‚ö†Ô∏è  TROP D'ERREURS D√âTECT√âES ({error_count}) - ANALYSE D√âTAILL√âE:")
                    should_analyze = True

            if should_analyze:
                # Analyse d√©taill√©e
                run_single_test_with_analysis(elements, relations, implementations)

            print("-" * 70)

    print("\n‚úÖ Tous les tests sont termin√©s.")
# --- Ex√©cution principale ---

if __name__ == "__main__":
    n_elements_list = [5_000, 25_000, 125_000, 750_000, 1_500_000, 2_500_000, 10_500_000]
    n_relations_factors = [1000, 1 / 20, 1 / 10, 1 / 5, 1 / 2]

    implementations = {
        "na√Øve": partition_par_unions_naive,
        "impl1": partition_par_unions_1,
        "impl2": partition_par_unions_2,
        "impl3": partition_par_unions_3,
        "impl4": partition_par_unions_4,
        "impl5": partition_par_unions_5,
    }

    elements, relations = generate_test_data(5000, 1000)
    run_single_test_with_analysis(elements, relations, implementations)

    # Ou utiliser la version modifi√©e des tests avec analyse p√©riodique
    run_tests_with_detailed_analysis(implementations, n_elements_list, n_relations_factors)

#    run_tests(implementations, n_elements_list, n_relations_factors)
